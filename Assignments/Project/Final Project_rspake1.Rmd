---
title: "Final Project_rspake1"
author: "Ryan Spake"
date: "5/8/2022"
output: html_document
---
## Final Project

In this project, we have found a dataset on Kaggle that tracks both Dow Jones Industrial Average (DJIA) performance and reddit News articles from r/worldnews subreddit.

The objective of this project is to build a Neural Network that follows the stock market performance and connects it to the top 25 trending news articles on that day to predict daily performance of stocks based off of the news headlines that day. 

## Getting Started

First thing we need to do is call our libraries and pull the combined table that connects top 25 Reddit news to the date (Reddit_df) and the table with daily stock information (DOW_df).

Note:
In the Reddit_df, a "label" is supplied that indicates if the stock market declined that day (0) and if it stayed the same or did better (1).

**First We will be doing some necessary cleaning before we build out model**

```{r, }
library(keras)
library(caret)
library(dplyr)
library(tm)

Reddit_df <- read.csv("C:/Users/rspake1/Desktop/Class work/Advanced machine learning/Final/Combined_News_DJIA/Combined_News_DJIA.csv")
Dow_df <-  read.csv("C:/Users/rspake1/Desktop/Class work/Advanced machine learning/Final/Combined_News_DJIA/upload_DJIA_table.csv")

set.seed(23)

Combined_df <- Dow_df
Rating_df <- Reddit_df[,2:3]
Reddit_df <- Reddit_df[,3:27]


vars <- c(names(Reddit_df[,c(1:5)]))
Rating_df$all_news <- apply(Reddit_df[,vars],1,paste,collapse="-")
colnames(Rating_df)[3] <- "First5"
```

With our data and libraries called, we can now tokenize our text data and build our models.

```{r}
maxlen <- 100
training_samples <- 200
validation_samples <- 500
max_features <- 500
set.seed(23)

tokenizer <- text_tokenizer(num_words = max_features) %>% 
  fit_text_tokenizer(Rating_df$First5)

sequences <- texts_to_sequences(tokenizer, Rating_df$First5)

word_index = tokenizer$word_index
cat("Found", length(word_index), "unique tokens.\n")

data <- pad_sequences(sequences, maxlen = maxlen)

labels <- as.array(Rating_df$Label)
cat("Shape of data tensor:", dim(data), "\n")
cat('Shape of label tensor:', dim(labels), "\n")

indices <- sample(1:nrow(data))
training_indices <- indices[1:training_samples]
validation_indices <- indices[(training_samples + 1): 
                              (training_samples + validation_samples)]

Redx_train <- data[training_indices,]
Redy_train <- labels[training_indices]

Redx_val <- data[validation_indices,]
Redy_val <- labels[validation_indices]
```

```{r}
embedding_dim <- 100

model <- keras_model_sequential() %>%
  layer_embedding(input_dim = max_features, output_dim = embedding_dim,
                  input_length = maxlen) %>%
  layer_flatten() %>%
  layer_dropout(rate = 0.5) %>%
  layer_dense(units = 16, activation = "relu") %>% 
  layer_dropout(rate = 0.5) %>%
  layer_dense(units = 1, activation = "sigmoid")
```

```{r}
summary(model)
```

```{r}
model %>% compile(
  optimizer = "rmsprop",
  loss = "binary_crossentropy",
  metrics = c("acc")
)

history <- model %>% fit(
  Redx_train, Redy_train,
  epochs = 10,
  batch_size = 100,
  validation_dat = list(Redx_val, Redy_val)
)
```
This model's best performance is at 4 epochs where we have a validation loss of 69% and accuracy of 53%.



```{r}
save_model_weights_hdf5(model, "Final_fixed.h5")
```

## Using different techniques


First we are going to increase the number of headlines that the machine is taing as an input. Then we will increase the total sample sizes and max lengths. 
```{r}
vars2 <- c(names(Reddit_df[,c(1:25)]))
Rating_df$all_news <- apply(Reddit_df[,vars],1,paste,collapse="-")
colnames(Rating_df)[4] <- "all_news"
```



```{r}
maxlen <- 500
training_samples <- 600
validation_samples <- 1000
max_features <- 1000
set.seed(23)

tokenizer <- text_tokenizer(num_words = max_features) %>% 
  fit_text_tokenizer(Rating_df$all_news)

sequences <- texts_to_sequences(tokenizer, Rating_df$all_news)

word_index = tokenizer$word_index
cat("Found", length(word_index), "unique tokens.\n")

data <- pad_sequences(sequences, maxlen = maxlen)

labels <- as.array(Rating_df$Label)
cat("Shape of data tensor:", dim(data), "\n")
cat('Shape of label tensor:', dim(labels), "\n")

indices <- sample(1:nrow(data))
training_indices <- indices[1:training_samples]
validation_indices <- indices[(training_samples + 1): 
                              (training_samples + validation_samples)]

Redx_train <- data[training_indices,]
Redy_train <- labels[training_indices]

Redx_val <- data[validation_indices,]
Redy_val <- labels[validation_indices]
```

```{r}
embedding_dim <- 100

model2 <- keras_model_sequential() %>%
  layer_embedding(input_dim = max_features, output_dim = embedding_dim,
                  input_length = maxlen) %>%
  layer_flatten() %>%
  layer_dropout(rate = 0.5) %>%
  layer_dense(units = 16, activation = "relu") %>% 
  layer_dropout(rate = 0.5) %>%
  layer_dense(units = 1, activation = "sigmoid")
```
In this model, we are increasing the number of inputs to see the effect it has on the model.
```{r}
summary(model2)
```

```{r}
model2 %>% compile(
  optimizer = "rmsprop",
  loss = "binary_crossentropy",
  metrics = c("acc")
)

history2 <- model2 %>% fit(
  Redx_train, Redy_train,
  epochs = 20,
  batch_size = 32,
  validation_dat = list(Redx_val, Redy_val)
)
```



This model instantly leveled out by epoch 4, leaving us with a validation accuracy of 53% and a loss of 69%. As we can see, increasing the input variable did not help our prediction model.


## Using LSTM
```{r}
embedding_dim <- 100

model3 <- keras_model_sequential() %>%
  layer_embedding(input_dim = max_features, output_dim = embedding_dim,
                  input_length = maxlen) %>%
  layer_lstm(units = 32) %>%
  layer_flatten() %>%
  layer_dropout(rate = 0.5) %>%
  layer_dense(units = 16, activation = "relu") %>% 
  layer_dropout(rate = 0.5) %>%
  layer_dense(units = 1, activation = "sigmoid")

model3 %>% compile(
  optimizer = "rmsprop",
  loss = "binary_crossentropy",
  metrics = c("acc")
)

history3 <- model3 %>% fit(
  Redx_train, Redy_train,
  epochs = 20,
  batch_size = 32,
  validation_dat = list(Redx_val, Redy_val)
)
```

In this last model we can see that it performs well up until 4 epochs. Topping out with an accuracy of 53% and a loss of 69%, this model will correctly predict stock market trends based off of Reddit activity about half of the time.

Even after changing input volume and model shape, the performance does not make any significant changes. 

From both of these models we can see that It is difficult to predict market trends based off of social media news headlines alone. More data would be necessary to perform a more clean and clear analysis on such a topic.
```{r}
save_model_weights_hdf5(model2, "Final_fixed2.h5")
save_model_weights_hdf5(model3, "Final_fixed3.h5")






